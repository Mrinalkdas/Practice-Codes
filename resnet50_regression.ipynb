{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib as plt\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "from fastai_utils import *\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "from model_utils import *\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "os.chdir('/data/data_backup_affine/Data_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "nb_epoch = 2\n",
    "train_data_path = \"./train\"\n",
    "validation_data_path = \"./val\"\n",
    "test_path = \"./final_test\"\n",
    "annotation_path = \"./final_data_annotation_with_col8.csv\"\n",
    "model_weights_path = \"./models/model_weights/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "img_channels = 3\n",
    "img_rows = 224\n",
    "img_cols = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_split_1 = 'res4a_branch2a'\n",
    "model_split_2 = 'fc_start'\n",
    "\n",
    "## Defining model name\n",
    "date = datetime.now().strftime(\"%m%d%Y\")\n",
    "model_name = 'resnet50_regression'\n",
    "save_model_path = './models/iterations/' + date + '_' + model_name + '/'\n",
    "if not os.path.exists(save_model_path):\n",
    "    os.mkdir(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>database</th>\n",
       "      <th>key</th>\n",
       "      <th>accident_no</th>\n",
       "      <th>img_name</th>\n",
       "      <th>comment</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>filename</th>\n",
       "      <th>accident_code</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Data_DBCar_Cases</td>\n",
       "      <td>130010</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke</td>\n",
       "      <td>AN_03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke_AN_03</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke_01FREE2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Data_DBCar_Cases</td>\n",
       "      <td>130010</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke</td>\n",
       "      <td>AN_15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke_AN_15</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke_01FREE2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Data_DBCar_Cases</td>\n",
       "      <td>130010</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke</td>\n",
       "      <td>AN_16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke_AN_16</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke_01FREE2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Data_DBCar_Cases</td>\n",
       "      <td>130010</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke</td>\n",
       "      <td>AN_17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke_AN_17</td>\n",
       "      <td>130010DV X218 frontal gegen Leitplanke_01FREE2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Data_DBCar_Cases</td>\n",
       "      <td>130032</td>\n",
       "      <td>130032DV C204 Frontalkollision schleudernd geg...</td>\n",
       "      <td>DA_01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>E</td>\n",
       "      <td>W</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>130032DV C204 Frontalkollision schleudernd geg...</td>\n",
       "      <td>130032DV C204 Frontalkollision schleudernd geg...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          database     key  \\\n",
       "0           1  Data_DBCar_Cases  130010   \n",
       "1           2  Data_DBCar_Cases  130010   \n",
       "2           3  Data_DBCar_Cases  130010   \n",
       "3           4  Data_DBCar_Cases  130010   \n",
       "4           5  Data_DBCar_Cases  130032   \n",
       "\n",
       "                                         accident_no img_name comment  Col2  \\\n",
       "0             130010DV X218 frontal gegen Leitplanke    AN_03     NaN     1   \n",
       "1             130010DV X218 frontal gegen Leitplanke    AN_15     NaN     1   \n",
       "2             130010DV X218 frontal gegen Leitplanke    AN_16     NaN     1   \n",
       "3             130010DV X218 frontal gegen Leitplanke    AN_17     NaN     1   \n",
       "4  130032DV C204 Frontalkollision schleudernd geg...    DA_01     NaN     1   \n",
       "\n",
       "  Col3 Col4 Col5 Col6  Col7  Col8  \\\n",
       "0    F    R    E    E     2  60.0   \n",
       "1    F    R    E    E     2  60.0   \n",
       "2    F    R    E    E     2  60.0   \n",
       "3    F    R    E    E     2  60.0   \n",
       "4    F    D    E    W     3  65.0   \n",
       "\n",
       "                                            filename  \\\n",
       "0       130010DV X218 frontal gegen Leitplanke_AN_03   \n",
       "1       130010DV X218 frontal gegen Leitplanke_AN_15   \n",
       "2       130010DV X218 frontal gegen Leitplanke_AN_16   \n",
       "3       130010DV X218 frontal gegen Leitplanke_AN_17   \n",
       "4  130032DV C204 Frontalkollision schleudernd geg...   \n",
       "\n",
       "                                       accident_code  split  \n",
       "0     130010DV X218 frontal gegen Leitplanke_01FREE2  train  \n",
       "1     130010DV X218 frontal gegen Leitplanke_01FREE2  train  \n",
       "2     130010DV X218 frontal gegen Leitplanke_01FREE2  train  \n",
       "3     130010DV X218 frontal gegen Leitplanke_01FREE2  train  \n",
       "4  130032DV C204 Frontalkollision schleudernd geg...  train  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading Data Annotation\n",
    "data = pd.read_csv(annotation_path, encoding = 'latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['180060DV X253 frontal gegen Bahnbetriebsh√§uschen', '150060DV S212 Polizei Seitenkollision durch Passat und frontal gegen Mast', '140052DV C117 Seitenkollision durch Vito und seitlich gegen Mast']\n"
     ]
    }
   ],
   "source": [
    "unq_acc = data[['accident_no','accident_code']].drop_duplicates()\n",
    "accident_name = unq_acc.loc[unq_acc['accident_no'].duplicated(),'accident_no']\n",
    "print(list(accident_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing dupicate accidents ... \n",
      " (557, 16)\n",
      "after removing nulls in col8 .. \n",
      " (499, 16)\n"
     ]
    }
   ],
   "source": [
    "data = data.loc[~data['accident_no'].isin(accident_name),:]\n",
    "print('after removing dupicate accidents ... \\n', data.shape)\n",
    "data = data.loc[~pd.isnull(data['Col8']),:]\n",
    "print('after removing nulls in col8 .. \\n', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading Data Annotation\n",
    "data['filename'] = data['filename'].apply(lambda x: x+'.jpg')\n",
    "columns = 'Col8'\n",
    "    \n",
    "## Preparing dataframe that will be used as an input to flow_from_directory\n",
    "image_files = pd.DataFrame({'filename': data['filename'], 'targets':data[columns]})\n",
    "image_files.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining augmentation techniques\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        # apply the following augmenters to most images\n",
    "        # iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "        iaa.OneOf([\n",
    "            sometimes(iaa.CropAndPad(percent=(-0.05, 0.1))), # zoom in\n",
    "            sometimes(iaa.Affine(scale={\"x\": (0.6, 1.2), \"y\": (0.6, 1.2)})) # zoom out\n",
    "        ]), \n",
    "        # execute 1 to 2 of the following augmenters per image\n",
    "        sometimes(iaa.Affine(translate_px={\"x\": (0, 25), \"y\": (0, 25)}, # horizontal/vertical shift\n",
    "                            rotate = (-25, 25),\n",
    "                            shear = (-15, 15))), \n",
    "        iaa.SomeOf((1, 4),\n",
    "                   [iaa.OneOf([\n",
    "                       iaa.PerspectiveTransform(scale=(0.01, 0.07)),\n",
    "                       iaa.Sharpen(alpha=(0, 0.5), lightness=(0.75, 1.5)),\n",
    "                       iaa.PiecewiseAffine((0.0, 0.01)), # local distortions\n",
    "                       iaa.GaussianBlur(sigma=(0, 0.7))\n",
    "                   ]),\n",
    "                    sometimes(iaa.Dropout((0.01, 0.02), per_channel=0.5)),\n",
    "                    iaa.AdditiveGaussianNoise(loc=32, scale=0.01*255), # white noise\n",
    "                    iaa.Add((-20, 50)), # brightness\n",
    "                    iaa.OneOf([\n",
    "                        iaa.LinearContrast(alpha=(0.5,1.2), per_channel = True),\n",
    "                        iaa.ContrastNormalization((0.5, 1.0))\n",
    "                    ])\n",
    "            ], random_order=True)\n",
    "    ], random_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431 Training Images\n",
      "Found 431 images.\n",
      "68 Validation Images\n",
      "Found 68 images.\n"
     ]
    }
   ],
   "source": [
    "## Creating image generators for train and validation\n",
    "img_train_gen = ImageDataGenerator(rescale=1/255, preprocessing_function = seq.augment_image)\n",
    "img_val_gen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "## Splitting annotated images into training and validation\n",
    "## train\n",
    "train_ids = list(set(data.loc[data['split'] == 'train', 'filename']))\n",
    "train_df = image_files.loc[image_files['filename'].isin(train_ids),:].drop_duplicates().reset_index(drop = True)\n",
    "print(str(len(train_ids)) + ' Training Images')\n",
    "train_gen = img_train_gen.flow_from_dataframe(train_df, train_data_path,\n",
    "                                  x_col = 'filename', y_col = 'targets',\n",
    "                                  has_ext = True, target_size = (img_rows, img_cols),\n",
    "                                  color_mode = 'rgb', batch_size = bs, class_mode = 'other')\n",
    "\n",
    "## validation\n",
    "val_ids = list(set(data.loc[data['split'] == 'val', 'filename']))\n",
    "val_df = image_files.loc[image_files['filename'].isin(val_ids),:].drop_duplicates().reset_index(drop = True)\n",
    "print(str(len(val_ids)) + ' Validation Images')\n",
    "val_gen = img_train_gen.flow_from_dataframe(val_df, validation_data_path,\n",
    "                                  x_col = 'filename', y_col = 'targets',\n",
    "                                  has_ext = True, target_size = (img_rows, img_cols),\n",
    "                                  color_mode = 'rgb', batch_size = bs, class_mode = 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model, img_channels, img_w, img_h):    \n",
    "    \"\"\"Builds and returns a learning model.\n",
    "    img_channels: the number of channels in the input images (1 for grayscale,\n",
    "        or 3 for RGB).\n",
    "    img_w: the width (in pixels) of the input images.\n",
    "    img_h: the height of the input images.\n",
    "    classes_list: the number of classes that the data will have (for each code) - this dictates\n",
    "        the number of values produced in the output layer.\n",
    "    Returns:\n",
    "    A deep neural network model.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu', name = 'fc_start')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfreezing all layers ...\n",
      "\n",
      "Epoch 1/2\n",
      "7/7 [==============================] - 97s 14s/step - loss: 2258.7995 - acc: 0.0045 - val_loss: 9378.0561 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 9378.05607, saving model to ./models/iterations/03222019_resnet50_regression/resnet50_regression_checkpoint.h5\n",
      "Epoch 2/2\n",
      "7/7 [==============================] - 65s 9s/step - loss: 1326.5105 - acc: 0.0029 - val_loss: 5495.2629 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss improved from 9378.05607 to 5495.26293, saving model to ./models/iterations/03222019_resnet50_regression/resnet50_regression_checkpoint.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "## Defining the input image size\n",
    "SHAPE = (img_rows, img_cols, img_channels)\n",
    "\n",
    "base_model = ResNet50(include_top=False, weights=None, input_shape = SHAPE)\n",
    "base_model.load_weights(model_weights_path)\n",
    "\n",
    "# print('freezing entire base model ...\\n')\n",
    "# # Freezing entire base model\n",
    "# for layer in base_model.layers[:]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "model_output = build_model(base_model, img_channels, img_rows, img_cols)\n",
    "model = Model(inputs = base_model.input, outputs = model_output)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(save_model_path + model_name +'_checkpoint.h5',\n",
    "                                   monitor='val_loss', \n",
    "                                   mode = 'auto', save_best_only=True, verbose=2)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode = 'auto',patience = 5, verbose=2)\n",
    "# For cyclical Learning Rate\n",
    "sched = LR_Cycle(iterations = np.ceil(len(train_ids)/bs),\n",
    "                 cycle_mult = 2)\n",
    "\n",
    "cbks = [model_checkpoint,early_stopping,sched]\n",
    "# cbks1 = [model_checkpoint,early_stopping]\n",
    "# cbks2 = [model_checkpoint,early_stopping, sched]\n",
    "\n",
    "# opt = SGD(lr = 5 * 1e-2, momentum = 0.9)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=opt,\n",
    "#               metrics=[f1,\"accuracy\"])\n",
    "\n",
    "# print('running 2 epochs with non-trainable base layers ...\\n')\n",
    "# history = model.fit_generator(train_gen,\n",
    "#                               steps_per_epoch = np.ceil(len(train_ids)/bs),\n",
    "#                               epochs = 2,\n",
    "#                               validation_data = val_gen,\n",
    "#                               validation_steps = np.ceil(len(val_ids)/bs),\n",
    "#                               use_multiprocessing = True,\n",
    "#                               callbacks = cbks1)\n",
    "\n",
    "\n",
    "\n",
    "print('unfreezing all layers ...\\n')\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# For Differential Learning Rate\n",
    "split_layer_1 = [layer for layer in model.layers if layer.name == model_split_1][0]\n",
    "split_layer_2 = [layer for layer in model.layers if layer.name == model_split_2][0]\n",
    "\n",
    "opt = Adam_dlr(split_l1 = split_layer_1,\n",
    "              split_l2 = split_layer_2,\n",
    "              lr = [1e-10, 1e-7, 5*1e-3])\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch = np.ceil(len(train_ids)/bs),\n",
    "                              epochs = nb_epoch,\n",
    "                              validation_data = val_gen,\n",
    "                              validation_steps = np.ceil(len(val_ids)/bs),\n",
    "                              use_multiprocessing = True,\n",
    "                              callbacks = cbks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 3.9354347467422484 minutes\n"
     ]
    }
   ],
   "source": [
    "## Save model and weights\n",
    "model_json = model.to_json()\n",
    "with open(save_model_path + model_name + '_model.json', \"w\", encoding = 'utf-8') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(save_model_path + model_name + '_weights.h5')\n",
    "\n",
    "#Calculate execution time\n",
    "end = time.time()\n",
    "dur = end-start\n",
    "\n",
    "if dur<60:\n",
    "    print(\"Execution Time:\",dur,\"seconds\")\n",
    "elif dur>60 and dur<3600:\n",
    "    dur=dur/60\n",
    "    print(\"Execution Time:\",dur,\"minutes\")\n",
    "else:\n",
    "    dur=dur/(60*60)\n",
    "    print(\"Execution Time:\",dur,\"hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 test images found ...\n",
      "loading the model ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Reading the true annotations\n",
    "data = pd.read_csv(annotation_path, encoding = 'latin-1')\n",
    "columns = 'Col8'\n",
    "data = data.loc[~pd.isnull(data['Col8']),:]\n",
    "\n",
    "image_names = [i for i in os.listdir(test_path) if 'Thumbs' not in i]\n",
    "\n",
    "data['filename'] = data['filename'] + '.jpg'\n",
    "test_df = data.loc[data['filename'].isin(image_names)].drop_duplicates().reset_index(drop = True)\n",
    "print(test_df.shape[0], 'test images found ...')\n",
    "\n",
    "model_path = save_model_path\n",
    "\n",
    "## Load model and weights\n",
    "print('loading the model ...\\n')\n",
    "json_file = open(model_path + 'resnet50_regression_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights(model_path + 'resnet50_regression_checkpoint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining augmentation techniques for Test Time Augmentation\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        # apply the following augmenters to most images\n",
    "        iaa.OneOf([\n",
    "            sometimes(iaa.CropAndPad(percent=(-0.05, 0.1))), # zoom in\n",
    "            sometimes(iaa.Affine(scale={\"x\": (0.6, 1.2), \"y\": (0.6, 1.2)})) # zoom out\n",
    "        ]), \n",
    "        # execute 1 to 2 of the following augmenters per image\n",
    "        sometimes(iaa.Affine(translate_px={\"x\": (0, 10), \"y\": (0, 10)},\n",
    "                            rotate = (-25, 25)\n",
    "                            )\n",
    "                 ), # horizontal/vertical shift\n",
    "        iaa.SomeOf((1, 4),\n",
    "                   [iaa.OneOf([\n",
    "                       iaa.Sharpen(alpha=(0, 0.5), lightness=(0.75, 1.5)),\n",
    "                       iaa.GaussianBlur(sigma=(0, 0.7))\n",
    "                   ]),\n",
    "                    sometimes(iaa.Dropout((0.01, 0.02), per_channel=0.5)),\n",
    "                    iaa.AdditiveGaussianNoise(loc=32, scale=0.01*255), # white noise\n",
    "                    iaa.Add((-20, 50)), # brightness\n",
    "                    iaa.OneOf([\n",
    "                        iaa.LinearContrast(alpha=(0.5,1.2), per_channel = True),\n",
    "                        iaa.ContrastNormalization((0.5, 1.0))\n",
    "                    ])\n",
    "            ], random_order=True)\n",
    "    ], random_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get actual code\n",
    "y_true = np.array(test_df[columns])\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get predicted code\n",
    "pred = []\n",
    "for ind in range(test_df.shape[0]):\n",
    "\n",
    "    image_n = test_df.loc[ind, 'filename']\n",
    "    ## Read Image\n",
    "    img = cv2.cvtColor(cv2.imread(test_path + '/' + image_n), cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (img_rows, img_cols))\n",
    "\n",
    "    images_l = []\n",
    "    ## Original Image\n",
    "    images_l.append(img/255)\n",
    "    ## Augmented Images\n",
    "    for aug in range(1):\n",
    "        images_l.append(seq.augment_image(img)/255)\n",
    "\n",
    "    img_predictions = []\n",
    "    for image in images_l:\n",
    "        img_predictions.append(model.predict(image.reshape((-1,img_rows, img_cols, 3)))[0,0])\n",
    "    pred.append(np.round(np.mean(img_predictions),0))\n",
    "    \n",
    "y_pred = np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.014\n",
      "MAPE:  66.893\n"
     ]
    }
   ],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "## Accuracy\n",
    "print('MAPE: ',np.round(mean_absolute_percentage_error(y_true, y_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
