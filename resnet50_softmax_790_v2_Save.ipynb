{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras import callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastai_utils import *\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "os.chdir('/data/Affinebkp150319/Affine')\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/data/data_backup_affine/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#config = tf.ConfigProto(device_count = {'GPU':0})\n",
    "#sess = tf.Session(config=config)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 17\n",
    "bs = 32\n",
    "img_size = 224\n",
    "\n",
    "model_split_1 = 'res4a_branch2a'\n",
    "model_split_2 = 'fc_start'\n",
    "\n",
    "date = datetime.now().strftime(\"%m%d%Y\")\n",
    "counter = str(int(np.round(start)))\n",
    "model_name = 'resnet50_790split_6_class_SGD_224size_notrain3_conv_dropout0.2_clr_bn_imgaug_' + counter\n",
    "print(model_name)\n",
    "\n",
    "train_data_path = './Data_v2/final_train/'\n",
    "validation_data_path = './Data_v2/final_validation/'\n",
    "# test_data_path = './Data_v2/test/'\n",
    "data_annotation_csv_path = './Data_v2/'\n",
    "model_weights_no_top = './models/model_weights/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "save_model_path = './models/iterations/'+ date +'/' + model_name + '/'\n",
    "tf_log_path = './tf-log/new_data/'\n",
    "\n",
    "if not os.path.exists('./models/iterations/'+ date +'/'):\n",
    "    os.mkdir('./models/iterations/'+ date +'/')\n",
    "    \n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "classes_2 = [1, 2, 3, 6, 8, 9, 10, 11, 12]\n",
    "classes_3 = ['F', 'L', 'R']\n",
    "classes_4 = ['B', 'C', 'D', 'F', 'L', 'P', 'R', 'Y', 'Z']\n",
    "# classes_4 = ['B', 'D', 'L', 'R', 'Y', 'Z']\n",
    "classes_5 = ['A', 'E', 'L', 'M']\n",
    "classes_6 = ['E', 'N', 'S', 'W']\n",
    "classes_7 = [1, 2, 3, 4, 5, 6, 8, 9]\n",
    "# classes_7 = [1, 2, 3]\n",
    "\n",
    "# classes = [classes_2, classes_3, classes_4, classes_5, classes_6, classes_7]\n",
    "classes = [classes_3, classes_4, classes_5, classes_6, classes_7]\n",
    "\n",
    "tot_num_classes = np.sum(len(c) for c in classes)\n",
    "\n",
    "split_idx = [np.sum(len(classes[j]) for j in range(i)) for i in range(len(classes))][1:]\n",
    "print(split_idx)\n",
    "\n",
    "# loss_wt = {}\n",
    "# # for i in range(len(classes)):\n",
    "# #     loss_wt['col'+str(i+2)] = np.round(len(classes[i])/tot_num_classes, 3)\n",
    "\n",
    "# losses = [1,3,1,1,1,3]\n",
    "# loss_wt = dict(zip(['col'+str(i+2) for i in range(len(classes))], losses))\n",
    "# print(loss_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_annotation_csv_path + 'Data_Annotation_v6.csv', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def one_hot_categorical(x, classes):\n",
    "    \n",
    "    lb_enc = LabelEncoder().fit(np.array(classes).reshape(-1,1))\n",
    "    label_encoded = lb_enc.transform(x)\n",
    "    \n",
    "    recip_freq  = len(label_encoded)/ (len(lb_enc.classes_) * np.bincount(label_encoded).astype(np.float64))\n",
    "    recip_freq[recip_freq == np.inf] = 0\n",
    "    weight = recip_freq[lb_enc.transform(lb_enc.classes_)]    \n",
    "    cw_dict = dict(zip(lb_enc.transform(lb_enc.classes_),np.round(weight,4)))\n",
    "    \n",
    "    enc = OneHotEncoder(sparse=False).fit(lb_enc.transform(np.array(classes).reshape(-1,1)).reshape(-1,1))\n",
    "    enc.transform(label_encoded.reshape(-1,1))\n",
    "    return enc.transform(label_encoded.reshape(-1,1)), cw_dict, lb_enc\n",
    "\n",
    "y_2, cw_2, lb_2 = one_hot_categorical(data['Col2'],classes_2)    \n",
    "y_3, cw_3, lb_3  = one_hot_categorical(data['Col3'],classes_3)\n",
    "# Including only side crash code \n",
    "# Not including R, C, L\n",
    "y_4, cw_4, lb_4  = one_hot_categorical(data['Col4'],classes_4)\n",
    "y_5, cw_5, lb_5  = one_hot_categorical(data['Col5'],classes_5)\n",
    "y_6, cw_6, lb_6  = one_hot_categorical(data['Col6'],classes_6)\n",
    "y_7, cw_7, lb_7  = one_hot_categorical(data['Col7'],classes_7)\n",
    "\n",
    "# lb = [lb_2, lb_3, lb_4, lb_5, lb_6, lb_7]\n",
    "lb = [lb_3, lb_4, lb_5, lb_6, lb_7]\n",
    "\n",
    "# cw = dict(zip(['col'+str(i+2) for i in range(len(classes))], [cw_2, cw_3, cw_4, cw_5, cw_6, cw_7]))\n",
    "# print(cw)\n",
    "\n",
    "# Create multi-label targets\n",
    "# targets = np.concatenate((y_2, y_3, y_4, y_5, y_6, y_7), axis = 1)\n",
    "targets = np.concatenate((y_3, y_4, y_5, y_6, y_7), axis = 1)\n",
    "\n",
    "image_files = pd.DataFrame(targets)\n",
    "image_files['filename'] = data['filename'].apply(lambda x: x+'.jpg')\n",
    "print()\n",
    "print('Found ' + str(image_files.shape[0]) + ' annotated images')\n",
    "print(image_files.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [i.split('/')[-1].strip() for i in glob.glob(train_data_path+'*.jpg')]\n",
    "train_df = image_files.loc[image_files['filename'].isin(train_ids),:].drop_duplicates().reset_index(drop = True)\n",
    "train_ids = list(train_df['filename'])\n",
    "print(len(train_ids), train_df.shape)\n",
    "\n",
    "val_ids = [i.split('/')[-1].strip() for i in glob.glob(validation_data_path+'*.jpg')]\n",
    "val_df = image_files.loc[image_files['filename'].isin(val_ids),:].drop_duplicates().reset_index(drop = True)\n",
    "val_ids = list(val_df['filename'])\n",
    "print(len(val_ids), val_df.shape)\n",
    "\n",
    "# test_ids = [i.split('/')[-1].strip() for i in glob.glob(test_data_path+'*.jpg')]\n",
    "# test_df = image_files.loc[image_files['filename'].isin(test_ids),:].reset_index(drop = True).drop_duplicates()\n",
    "# print(len(test_ids), test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_outputs(generator, idx):\n",
    "    while True:\n",
    "        data = next(generator)\n",
    "        x = data[0]\n",
    "        y = np.split(data[1], idx, axis=1)\n",
    "        yield x, y\n",
    "        \n",
    "def read_data(img_data_gen, base_dir, in_df, idx, path_col = 'filename',\n",
    "                        y_col = 'targets', batch_size = 8, n_classes = 38, size = 512):\n",
    "    \n",
    "    print(in_df.shape)\n",
    "    df_gen = img_data_gen.flow_from_dataframe(in_df, base_dir,\n",
    "                                              x_col = 'filename', y_col= list(range(n_classes)),\n",
    "                                              has_ext = True, target_size = (size, size),\n",
    "                                              color_mode = 'rgb', class_mode = 'other',\n",
    "                                              batch_size = batch_size)\n",
    "    \n",
    "    return split_outputs(df_gen, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "few_of = lambda aug: iaa.Sometimes(0.2, aug)\n",
    "\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "#         apply the following augmenters to most images\n",
    "#         iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "        iaa.OneOf([\n",
    "            sometimes(iaa.CropAndPad(percent=(-0.05, 0.1))), # zoom in\n",
    "            sometimes(iaa.Affine(scale={\"x\": (0.6, 1.2), \"y\": (0.6, 1.2)})) # zoom out\n",
    "        ]), \n",
    "        # execute 1 to 2 of the following augmenters per image\n",
    "        sometimes(iaa.Affine(translate_px={\"x\": (0, 25), \"y\": (0, 25)},\n",
    "                            rotate = (-25, 25),\n",
    "                            shear = (-15, 15))), # horizontal/vertical shift\n",
    "        iaa.SomeOf((1, 4),\n",
    "                   [iaa.OneOf([\n",
    "#                        iaa.AllChannelsCLAHE(clip_limit = (0.5, 0.8)),\n",
    "                       iaa.PerspectiveTransform(scale=(0.01, 0.07)),\n",
    "                       iaa.Sharpen(alpha=(0, 0.5), lightness=(0.75, 1.5)),\n",
    "                       iaa.PiecewiseAffine((0.0, 0.01)), # local distortions\n",
    "                       iaa.GaussianBlur(sigma=(0, 0.7))\n",
    "                   ]),\n",
    "                    sometimes(iaa.Dropout((0.005, 0.02), per_channel=0.5)),\n",
    "                    iaa.AdditiveGaussianNoise(loc=32, scale=0.01*255), # white noise\n",
    "                    iaa.Add((-20, 50)), # brightness\n",
    "#                     iaa.AddToHueAndSaturation((-20, 20)),\n",
    "#                     iaa.AllChannelsHistogramEqualization(),\n",
    "                    iaa.OneOf([\n",
    "                        iaa.LinearContrast(alpha=(0.5,1.2), per_channel = True),\n",
    "#                         iaa.LogContrast(gain=(0.5,0.8), per_channel=True),\n",
    "#                         iaa.GammaContrast(gamma=(0.4, 1.0)),\n",
    "                        iaa.ContrastNormalization((0.5, 1.0))\n",
    "                    ]),\n",
    "#                     few_of(iaa.Clouds()),\n",
    "                    few_of(iaa.Multiply((0.7, 1.5), per_channel = True))\n",
    "            ], random_order=True)\n",
    "    ], random_order=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_train_gen = ImageDataGenerator(rescale=1/255, preprocessing_function=seq.augment_image)\n",
    "img_val_gen = ImageDataGenerator(rescale=1/255)\n",
    "# img_test_gen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_gen = read_data(img_train_gen, train_data_path,train_df, idx = split_idx,\n",
    "                                batch_size = bs, n_classes = tot_num_classes, size = img_size)\n",
    "val_gen = read_data(img_val_gen,validation_data_path, val_df,idx = split_idx,\n",
    "                              batch_size = bs,  n_classes = tot_num_classes, size = img_size)\n",
    "# test_gen = read_data(img_test_gen, test_data_path,test_df,idx = split_idx,\n",
    "#                                batch_size = bs,  n_classes = tot_num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_gen:\n",
    "    break\n",
    "\n",
    "print(len(x), len(y), len(y[0]))\n",
    "print(x[0].shape)\n",
    "i = 0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100, 100))\n",
    "\n",
    "for i in range(9):\n",
    "    \n",
    "    fig.add_subplot(3, 3, i+1)\n",
    "    \n",
    "    # Extract corresponding CDC Code\n",
    "#     actual = [y[0][i], y[1][i], y[2][i], y[3][i], y[4][i], y[5][i]]\n",
    "    actual = [y[0][i], y[1][i], y[2][i], y[3][i], y[4][i]]\n",
    "    code = ''\n",
    "    for j,act in enumerate(actual):\n",
    "        code = code + str(lb[j].inverse_transform([np.argmax(act)])[0])\n",
    "                                                   \n",
    "    plt.imshow(x[i])\n",
    "    plt.title(code, fontsize = 70)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from resnet50 import ResNet50\n",
    "base_model = ResNet50(include_top=False, weights=None, input_shape = (img_size, img_size, 3))\n",
    "base_model.load_weights(model_weights_no_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing base model till first split layer\n",
    "# trainable = False\n",
    "# for i,layer in enumerate(base_model.layers[:]):\n",
    "#     try:\n",
    "#         if layer.name == model_split_1:\n",
    "#             trainable = True\n",
    "#     except:\n",
    "#         pass\n",
    "#     layer.trainable = trainable\n",
    "#     print(layer, layer.trainable)\n",
    "    \n",
    "# Freezing entire base model\n",
    "for layer in base_model.layers[:]:\n",
    "    layer.trainable = False\n",
    "    print(layer,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Conv2D\n",
    "from keras import regularizers\n",
    "\n",
    "x = base_model.output\n",
    "\n",
    "x = Conv2D(filters = 512,kernel_size = 3,strides=(1,1), name = 'fc_start')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# # Col 2\n",
    "# x2 = Conv2D(filters = 256,kernel_size = 3,strides=(1,1))(x)\n",
    "# x2 = GlobalAveragePooling2D()(x2)\n",
    "# x2 = Dense(1024, activation='relu')(x2)\n",
    "# x2 = Dropout(0.2)(x2)\n",
    "# x2 = BatchNormalization()(x2)\n",
    "# x2 = Dense(512, activation='relu')(x2)\n",
    "# x2 = Dropout(0.2)(x2)\n",
    "# out2 = Dense(len(classes_2), activation='softmax', name = 'col2')(x2)\n",
    "\n",
    "# Col 3\n",
    "x3 = GlobalAveragePooling2D()(x)\n",
    "x3 = Dense(1024, activation='relu')(x3)\n",
    "x3 = Dropout(0.2)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Dense(512, activation='relu')(x3)\n",
    "x3 = Dropout(0.2)(x3)\n",
    "out3 = Dense(len(classes_3), activation='softmax', name = 'col3')(x3)\n",
    "\n",
    "# Col 4\n",
    "x4 = Conv2D(filters = 256,kernel_size = 3,strides=(1,1))(x)\n",
    "x4 = GlobalAveragePooling2D()(x4)\n",
    "x4 = Dense(1024, activation='relu')(x4)\n",
    "x4 = Dropout(0.2)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Dense(512, activation='relu')(x4)\n",
    "x4 = Dropout(0.2)(x4)\n",
    "out4 = Dense(len(classes_4), activation='softmax', name = 'col4')(x4)\n",
    "\n",
    "# Col 5\n",
    "x5 = GlobalAveragePooling2D()(x)\n",
    "x5 = Dense(1024, activation='relu')(x5)\n",
    "x5 = Dropout(0.2)(x5)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Dense(512, activation='relu')(x5)\n",
    "x5 = Dropout(0.2)(x5)\n",
    "out5 = Dense(len(classes_5), activation='softmax', name = 'col5')(x5)\n",
    "\n",
    "# Col 6\n",
    "x6 = GlobalAveragePooling2D()(x)\n",
    "x6 = Dense(1024, activation='relu')(x6)\n",
    "x6 = Dropout(0.2)(x6)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = Dense(512, activation='relu')(x6)\n",
    "x6 = Dropout(0.2)(x6)\n",
    "out6 = Dense(len(classes_6), activation='softmax', name = 'col6')(x6)\n",
    "\n",
    "# Col 7\n",
    "x7 = Conv2D(filters = 256,kernel_size = 3,strides=(1,1))(x)\n",
    "x7 = GlobalAveragePooling2D()(x7)\n",
    "x7 = Dense(1024, activation='relu')(x7)\n",
    "x7 = Dropout(0.2)(x7)\n",
    "x7 = BatchNormalization()(x7)\n",
    "x7 = Dense(512, activation='relu')(x7)\n",
    "x7 = Dropout(0.2)(x7)\n",
    "out7 = Dense(len(classes_7), activation='softmax', name = 'col7')(x7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "# model = Model(inputs=base_model.input, outputs=[out2, out3, out4, out5, out6, out7])\n",
    "model = Model(inputs=base_model.input, outputs=[out3, out4, out5, out6, out7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "\n",
    "    TP = tf.count_nonzero(y_pred * y_true, axis=0, dtype = tf.float64)\n",
    "    FP = tf.count_nonzero(y_pred * (y_true - 1), axis=0, dtype = tf.float64)\n",
    "    FN = tf.count_nonzero((y_pred - 1) * y_true, axis=0, dtype = tf.float64)\n",
    "\n",
    "    add_dummy = lambda x: x + K.epsilon()\n",
    "    precision = TP /  tf.map_fn(add_dummy, (TP + FP))\n",
    "    recall = TP / tf.map_fn(add_dummy, (TP + FN))\n",
    "    f1 = 2 * precision * recall / tf.map_fn(add_dummy, (precision + recall))\n",
    "\n",
    "    weights = tf.reduce_sum(y_true, axis=0)\n",
    "    weights /= tf.reduce_sum(weights)\n",
    "    return tf.reduce_sum(f1 * weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred):\n",
    "    \n",
    "    gamma=2\n",
    "    alpha=0.25\n",
    "    eps = 1e-12\n",
    "    \n",
    "    y_pred = K.clip(y_pred, eps, 1.-eps) #improve the stability of the focal loss and see issues 1 for more information\n",
    "\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find LR\n",
    "\n",
    "def lr_finder(mod, train_generator, training_steps, optimizer, loss_type, metrics, lr_range = [1e-5, 0.1]):\n",
    "    \n",
    "    lr_find = LR_Find(iterations = training_steps,\n",
    "                      jump = 6,\n",
    "                      min_lr = lr_range[0],\n",
    "                      max_lr = lr_range[1])\n",
    "    mod.compile(loss = loss_type,\n",
    "                optimizer=optimizer,\n",
    "                metrics=metrics)\n",
    "    mod.fit_generator(train_generator,\n",
    "                        steps_per_epoch = training_steps,\n",
    "                        epochs = 1,\n",
    "                        workers = 1,\n",
    "                        callbacks = [lr_find])\n",
    "\n",
    "    fig=plt.figure(figsize=(15, 5))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "\n",
    "    # Show LR vs Loss\n",
    "    lr_find.plot()\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    \n",
    "    # Show LR vs Iterations\n",
    "    lr_find.plot_lr()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# lr_finder(mod = model,\n",
    "#           train_generator = train_gen,\n",
    "#           training_steps = np.ceil(len(train_ids)/bs),\n",
    "#           optimizer = Adam(),\n",
    "#           loss_type = 'categorical_crossentropy',\n",
    "#           metrics = [f1,\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "if not os.path.exists(save_model_path):\n",
    "    os.mkdir(save_model_path)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(save_model_path + date + '_checkpoint_' + model_name +'.h5',\n",
    "                                   monitor='val_loss', \n",
    "                                   mode = 'auto', save_best_only=True, verbose=2)\n",
    "\n",
    "tb_cb = callbacks.TensorBoard(log_dir=tf_log_path, histogram_freq=0)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode = 'auto',patience = 5, verbose=2)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode = 'auto',factor = 0.8, patience = 3, min_lr = 1e-4, verbose=2)\n",
    "\n",
    "# For cyclical Learning Rate\n",
    "sched = LR_Cycle(iterations = np.ceil(len(train_ids)/bs),\n",
    "                 cycle_mult = 2)\n",
    "\n",
    "# cbks = [model_checkpoint,early_stopping,reduce_lr]\n",
    "cbks1 = [model_checkpoint,early_stopping]\n",
    "cbks2 = [model_checkpoint,early_stopping, sched]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "opt = SGD(lr = 2 * 1e-2, momentum = 0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=[f1,\"accuracy\"])\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch = np.ceil(len(train_ids)/bs),\n",
    "                              epochs = 2,\n",
    "                              validation_data = val_gen,\n",
    "                              validation_steps = np.ceil(len(val_ids)/bs),\n",
    "                              use_multiprocessing = True,\n",
    "                              callbacks = cbks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# For Differential Learning Rate\n",
    "split_layer_1 = [layer for layer in model.layers if layer.name == model_split_1][0]\n",
    "split_layer_2 = [layer for layer in model.layers if layer.name == model_split_2][0]\n",
    "\n",
    "opt = SGD_dlr(split_l1 = split_layer_1,\n",
    "               split_l2 = split_layer_2,\n",
    "               lr = [1e-10, 1e-6, 1e-3],\n",
    "             momentum = 0.9)\n",
    "\n",
    "# opt = Adam(lr = 5 * 1e-3)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=[f1,\"accuracy\"])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=opt,\n",
    "#               metrics=[f1,\"accuracy\"],\n",
    "#               loss_weights = loss_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch = np.ceil(len(train_ids)/bs),\n",
    "                              epochs = n_epochs,\n",
    "                              validation_data = val_gen,\n",
    "                              validation_steps = np.ceil(len(val_ids)/bs),\n",
    "                              use_multiprocessing = True,\n",
    "                              callbacks = cbks2)\n",
    "\n",
    "# history = model.fit_generator(train_gen,\n",
    "#                               steps_per_epoch = np.ceil(len(train_ids)/bs),\n",
    "#                               epochs = n_epochs,\n",
    "#                               validation_data = val_gen,\n",
    "#                               validation_steps = np.ceil(len(val_ids)/bs),\n",
    "#                               use_multiprocessing = True,\n",
    "#                               callbacks = cbks,\n",
    "#                               class_weight = cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(save_model_path + date + '_' + model_name + '_model.json', \"w\", encoding = 'utf-8') as json_file:\n",
    "    json_file.write(model_json)\n",
    "# model.save(save_model_path + date + '_' + model_name + '_model.h5')\n",
    "model.save_weights(save_model_path + date + '_' + model_name + '_weights.h5')\n",
    "\n",
    "#Calculate execution time\n",
    "\n",
    "end = time.time()\n",
    "dur = end-start\n",
    "\n",
    "if dur<60:\n",
    "    print(\"Execution Time:\",dur,\"seconds\")\n",
    "elif dur>60 and dur<3600:\n",
    "    dur=dur/60\n",
    "    print(\"Execution Time:\",dur,\"minutes\")\n",
    "else:\n",
    "    dur=dur/(60*60)\n",
    "    print(\"Execution Time:\",dur,\"hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "print(save_model_path + date + '_' + model_name + '_model.json')\n",
    "json_file = open(save_model_path + date + '_' + model_name + '_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "model = model_from_json(loaded_model_json, \n",
    "                        custom_objects={'f1':f1})\n",
    "\n",
    "model.load_weights(save_model_path + date + '_checkpoint_' + model_name +'.h5')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=[f1,\"accuracy\"])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=opt,\n",
    "#               metrics=[f1,\"accuracy\"],\n",
    "#               loss_weights = loss_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model, generator, folder, n_steps, n_classes):\n",
    "    eval_metrics = model.evaluate_generator(generator, workers = 1, steps = n_steps)\n",
    "    print(folder + ' :')\n",
    "    print('Loss: ' + str(eval_metrics[0]))\n",
    "    print('Columnwise f1: ',str([eval_metrics[i] for i in range(1 + n_classes, 1 + 3*n_classes, 2)]))\n",
    "    print()\n",
    "    \n",
    "model_evaluation(model, train_gen, 'Train', n_steps = (len(train_ids)// bs + 1), n_classes = len(classes))\n",
    "model_evaluation(model, val_gen, 'Validation', n_steps = (len(val_ids)// bs + 1), n_classes = len(classes))\n",
    "# model_evaluation(model, test_gen, 'Test', n_steps = (len(test_ids)// bs + 1), n_classes = len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to add rules for predictions\n",
    "\n",
    "def get_predictions(model, base_dir, generator, in_df, idx, num_classes, n_steps, threshold = 0.5, size = 512):\n",
    "    \n",
    "    y_true = np.split(np.array(in_df.loc[:,list(range(num_classes))]), idx, axis=1)\n",
    "    print(np.array(in_df.loc[:,list(range(num_classes))]).shape)\n",
    "    true = [np.argmax(x, axis = 1) for x in y_true]\n",
    "    \n",
    "    pred = []\n",
    "    img_pred = []\n",
    "    for ind in range(in_df.shape[0]):\n",
    "        \n",
    "        images_l = []\n",
    "        img_predictions = []\n",
    "    \n",
    "        img_path = base_dir + in_df.loc[ind, 'filename']\n",
    "        img = cv2.resize(plt.imread(img_path), (size, size))\n",
    "        images_l.append(img/255)\n",
    "        \n",
    "        for aug in range(4):\n",
    "            images_l.append(seq.augment_image(img)/255)\n",
    "        for image in images_l:\n",
    "            img_predictions.append(model.predict(image.reshape((-1,size, size, 3))))\n",
    "        predictions = []\n",
    "        for c in range(len(classes)):\n",
    "            class_pred = np.array(([(img_predictions[j][c]).reshape(-1) for j in range(len(img_predictions))]))\n",
    "            predictions.append(np.argmax(np.mean(class_pred, axis = 0)))\n",
    "\n",
    "        img_pred.append(predictions) \n",
    "        pred = [np.array([i[j] for i in img_pred]) for j in range(len(classes))]\n",
    "    return (true, pred)\n",
    "\n",
    "\n",
    "print('Train:')\n",
    "train_true, train_pred = get_predictions(model, train_data_path, train_gen, train_df, idx = split_idx,\n",
    "                                         num_classes = tot_num_classes, n_steps = (len(train_ids)// bs + 1), size = img_size)\n",
    "print('\\n Validation:')\n",
    "val_true, val_pred = get_predictions(model, validation_data_path, val_gen, val_df, idx = split_idx,\n",
    "                                     num_classes = tot_num_classes, n_steps = (len(val_ids)// bs + 1), size = img_size)\n",
    "# test_true, test_pred = get_predictions(model, test_data_path, test_gen, test_df, idx = split_idx,\n",
    "#                                        num_classes = tot_num_classes, n_steps = (len(test_ids)// bs + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_classes(true, pred, lb):\n",
    "    true_classes = []\n",
    "    pred_classes = []\n",
    "    for i in range(len(lb)):\n",
    "        true_classes.append(lb[i].inverse_transform(true[i]))\n",
    "        pred_classes.append(lb[i].inverse_transform(pred[i]))\n",
    "    return true_classes, pred_classes\n",
    "\n",
    "train_true_classes, train_pred_classes = get_predicted_classes(train_true, train_pred, lb)\n",
    "val_true_classes, val_pred_classes = get_predicted_classes(val_true, val_pred, lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=False):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def plot_cm(true, pred, classes, i, le):\n",
    "\n",
    "    report = metrics.confusion_matrix(true, pred, labels = le.classes_)\n",
    "    plot_confusion_matrix(report,classes, title = 'col_' + str(i))\n",
    "    \n",
    "def plot_cm_all_codes(true, pred, list_classes, lb_enc):\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        plot_cm(true[i], pred[i], list_classes[i], i+2, lb_enc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_cm_all_codes(train_true_classes, train_pred_classes, classes, lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm_all_codes(val_true_classes, val_pred_classes, classes, lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_model_report(writer, folder, true, pred, classes):\n",
    "    \n",
    "    print('************' + folder + '*************')\n",
    "    total_samples = len(true[0])\n",
    "    start = 0\n",
    "    m = []\n",
    "    for i in range(len(classes)):\n",
    "        \n",
    "        class_true = true[i]\n",
    "        class_pred = pred[i]\n",
    "        \n",
    "        # Accuracy Metrics\n",
    "        prf = np.round(metrics.precision_recall_fscore_support(class_true, class_pred, average='weighted')[:-1],4)\n",
    "        acc = np.round(metrics.accuracy_score(class_true, class_pred),4)\n",
    "        metric_list = ['Col'+str(i+2)]\n",
    "        metric_list.extend(prf)\n",
    "        metric_list.extend([acc])\n",
    "        m.append(metric_list)\n",
    "        print('col_' + str(i+2) + ': ' + str(metric_list))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        report = metrics.confusion_matrix(class_true, class_pred, labels = lb[i].classes_)\n",
    "        df = pd.DataFrame(report, columns = lb[i].classes_, index = lb[i].classes_)\n",
    "        df.to_excel(writer,sheet_name='CM_' + folder ,startrow = start , startcol=0)   \n",
    "        \n",
    "        # Class Scores\n",
    "        df_class_score = pd.DataFrame(index = lb[i].classes_)\n",
    "        df_class_score['Precision'] = metrics.precision_score(class_true, class_pred, average = None, labels = lb[i].classes_)\n",
    "        df_class_score['Recall'] = metrics.recall_score(class_true, class_pred, average = None, labels = lb[i].classes_)\n",
    "        df_class_score['F1'] = metrics.f1_score(class_true, class_pred, average = None, labels = lb[i].classes_)\n",
    "        df_class_score.to_excel(writer, sheet_name = folder + '_score', startrow = start, startcol = 0)\n",
    "    \n",
    "        start = start + len(classes[i]) + 2\n",
    "        \n",
    "    overall_acc = (pd.DataFrame(true) == pd.DataFrame(pred)).all().sum() / total_samples\n",
    "#     print(folder + ' overall accuracy: ', str(np.round(overall_acc *100, 2)))\n",
    "    metrics_df = pd.DataFrame(m, columns = ['Col','Precision','Recall','F1','Accuracy'])\n",
    "    metrics_df.to_excel(writer, sheet_name=folder, index = False)\n",
    "    pd.DataFrame({'Accuracy' : [overall_acc],'Samples':[total_samples]}).to_excel(writer, sheet_name=folder, startrow = 8, index = False)\n",
    "    print()\n",
    "    \n",
    "    return writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Overall')\n",
    "print(save_model_path + date + '_' + model_name + '_report.xlsx')\n",
    "writer = pd.ExcelWriter(save_model_path + date + '_' + model_name + '_overall_report.xlsx', engine='xlsxwriter')\n",
    "writer = cm_model_report(writer, 'train', train_true_classes, train_pred_classes, classes)    \n",
    "writer = cm_model_report(writer, 'val', val_true_classes, val_pred_classes, classes)\n",
    "# writer = cm_model_report(writer, 'test', test_true, test_pred, classes) \n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_evaluation(writer, folder, type_crash, true, pred, classes):\n",
    "\n",
    "    idx = np.where(true[0] == type_crash)[0]\n",
    "    writer = cm_model_report(writer,\n",
    "                             folder,\n",
    "                             list(map(lambda x: x[idx], true)),\n",
    "                             list(map(lambda x: x[idx], pred)),\n",
    "                             classes)  \n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_crash = 'F'\n",
    "print(type_crash)\n",
    "print(save_model_path + date + '_' + model_name + '_' + type_crash + '_report.xlsx')\n",
    "writer = pd.ExcelWriter(save_model_path + date + '_' + model_name + '_' + type_crash + '_report.xlsx', engine='xlsxwriter')\n",
    "writer = get_type_evaluation(writer, 'train', type_crash, train_true_classes, train_pred_classes, classes)\n",
    "writer = get_type_evaluation(writer, 'val', type_crash, val_true_classes, val_pred_classes, classes)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_crash = 'L'\n",
    "print(type_crash)\n",
    "print(save_model_path + date + '_' + model_name + '_' + type_crash + '_report.xlsx')\n",
    "writer = pd.ExcelWriter(save_model_path + date + '_' + model_name + '_' + type_crash + '_report.xlsx', engine='xlsxwriter')\n",
    "writer = get_type_evaluation(writer, 'train', type_crash, train_true_classes, train_pred_classes, classes)\n",
    "writer = get_type_evaluation(writer, 'val', type_crash, val_true_classes, val_pred_classes, classes)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_crash = 'R'\n",
    "print(type_crash)\n",
    "print(save_model_path + date + '_' + model_name + '_' + type_crash + '_report.xlsx')\n",
    "writer = pd.ExcelWriter(save_model_path + date + '_' + model_name + '_' + type_crash + '_report.xlsx', engine='xlsxwriter')\n",
    "writer = get_type_evaluation(writer, 'train', type_crash, train_true_classes, train_pred_classes, classes)\n",
    "writer = get_type_evaluation(writer, 'val', type_crash, val_true_classes, val_pred_classes, classes)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actuals_predictions(split_df, actual_classes, predicted_classes, classes):\n",
    "    actuals = pd.DataFrame(actual_classes).T\n",
    "    actuals.columns = ['actual_' + str(i+3) for i in range(len(classes))]\n",
    "    predicted = pd.DataFrame(predicted_classes).T\n",
    "    predicted.columns = ['predicted_' + str(i+3) for i in range(len(classes))]\n",
    "    df = pd.concat([split_df['filename'], actuals, predicted], axis = 1)\n",
    "    return df\n",
    "\n",
    "writer = pd.ExcelWriter(save_model_path + date + '_' + model_name + 'actuals_vs_predicted.xlsx', engine='xlsxwriter')\n",
    "get_actuals_predictions(train_df, train_true_classes, train_pred_classes, classes).to_excel(writer, sheet_name='train')\n",
    "get_actuals_predictions(val_df, val_true_classes, val_pred_classes, classes).to_excel(writer, sheet_name='val')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc_dict = {}\n",
    "for i, c in enumerate(classes):\n",
    "    label_enc_dict[i] = LabelEncoder().fit(c)\n",
    "    \n",
    "val_images = validation_data_path + val_df['filename']\n",
    "train_images = train_data_path + train_df['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrect third column\n",
    "img_idx = np.where(train_true[0] != train_pred[0])[0]\n",
    "\n",
    "for i in img_idx:\n",
    "    \n",
    "    img_path = train_images[i]\n",
    "    img = cv2.resize(plt.imread(img_path), (img_size, img_size))/255\n",
    "    \n",
    "    img_pred = [train_pred[c][i] for c in range(len(classes))]\n",
    "    pred_code = [str(label_enc_dict[i].inverse_transform([img_pred[i]])[0]) for i in range(len(img_pred))]\n",
    "#     pred_code[0] = pred_code[0].zfill(2)\n",
    "    pred = ''.join(pred_code)\n",
    "\n",
    "    img_act = [train_true[c][i] for c in range(len(classes))]\n",
    "    act_code = [str(label_enc_dict[i].inverse_transform([img_act[i]])[0]) for i in range(len(img_act))]\n",
    "#     act_code[0] = act_code[0].zfill(2)\n",
    "    act = ''.join(act_code)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(plt.imread(img_path))\n",
    "    plt.title(('Actual: ' + act + ' ----- Predicted: ' + pred), fontsize = 20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrect third column\n",
    "img_idx = np.where(val_true[0] != val_pred[0])[0]\n",
    "\n",
    "for i in img_idx:\n",
    "    \n",
    "    img_path = val_images[i]\n",
    "    img = cv2.resize(plt.imread(img_path), (img_size, img_size))/255\n",
    "    \n",
    "    img_pred = [val_pred[c][i] for c in range(len(classes))]\n",
    "    pred_code = [str(label_enc_dict[i].inverse_transform([img_pred[i]])[0]) for i in range(len(img_pred))]\n",
    "#     pred_code[0] = pred_code[0].zfill(2)\n",
    "    pred = ''.join(pred_code)\n",
    "\n",
    "    img_act = [val_true[c][i] for c in range(len(classes))]\n",
    "    act_code = [str(label_enc_dict[i].inverse_transform([img_act[i]])[0]) for i in range(len(img_act))]\n",
    "#     act_code[0] = act_code[0].zfill(2)\n",
    "    act = ''.join(act_code)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(plt.imread(img_path))\n",
    "    plt.title(('Actual: ' + act + ' ----- Predicted: ' + pred), fontsize = 20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
